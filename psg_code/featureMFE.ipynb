{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  StartDate  duration  hy\n",
      "0  12:43:06     18.49  hy\n",
      "1  12:44:55     13.36  hy\n",
      "2  12:46:27     10.55  hy\n",
      "3  12:46:41     10.60  hy\n",
      "4  12:46:56     10.80  hy\n",
      "            StartDate  duration  hy                 endDate\n",
      "0 1900-01-01 12:43:06     18.49  hy 1900-01-01 12:43:24.490\n",
      "1 1900-01-01 12:44:55     13.36  hy 1900-01-01 12:45:08.360\n",
      "2 1900-01-01 12:46:27     10.55  hy 1900-01-01 12:46:37.550\n",
      "3 1900-01-01 12:46:41     10.60  hy 1900-01-01 12:46:51.600\n",
      "4 1900-01-01 12:46:56     10.80  hy 1900-01-01 12:47:06.800\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Đường dẫn đến tệp Excel\n",
    "file_path = './input/label.xlsx'\n",
    "\n",
    "# Đọc tệp Excel\n",
    "datatimeOA = pd.read_excel(file_path)\n",
    "\n",
    "# Hiển thị các dòng đầu tiên của DataFrame\n",
    "print(datatimeOA.head())\n",
    "datatimeOA['StartDate'] = pd.to_datetime(datatimeOA['StartDate'], format='%H:%M:%S')\n",
    "\n",
    "# Tính toán endDate bằng cách thêm duration (tính bằng giờ) vào StartDate\n",
    "datatimeOA['endDate'] = datatimeOA['StartDate'] + pd.to_timedelta(datatimeOA['duration'], unit='s')\n",
    "\n",
    "# Hiển thị các dòng đầu tiên của DataFrame sau khi tính toán\n",
    "print(datatimeOA.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatimeOA['endDate'] = pd.to_datetime(datatimeOA['endDate'], format='%H:%M:%S:%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           time     value\n",
      "0  00:37:05:162  0.148586\n",
      "1  00:37:05:167  0.148548\n",
      "2  00:37:05:172  0.148485\n",
      "3  00:37:05:177  0.148397\n",
      "4  00:37:05:182  0.148284\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Đường dẫn đến tệp Excel\n",
    "file_path = './input/flowcmh20.xlsx'\n",
    "\n",
    "# Đọc tệp Excel\n",
    "flowData = pd.read_excel(file_path)\n",
    "\n",
    "\n",
    "\n",
    "# Hiển thị các dòng đầu tiên của DataFrame\n",
    "print(flowData.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hour_to_12(dt):\n",
    "    if dt.hour == 0:\n",
    "        return dt.replace(hour=12)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     time     value  nhan\n",
      "0 1900-01-01 12:37:05.162  0.148586     0\n",
      "1 1900-01-01 12:37:05.167  0.148548     0\n",
      "2 1900-01-01 12:37:05.172  0.148485     0\n",
      "3 1900-01-01 12:37:05.177  0.148397     0\n",
      "4 1900-01-01 12:37:05.182  0.148284     0\n"
     ]
    }
   ],
   "source": [
    "flowData['time'] = pd.to_datetime(flowData['time'], format='%H:%M:%S:%f')\n",
    "flowData['time'] = flowData['time'].apply(convert_hour_to_12)\n",
    "flowData['nhan'] = 0\n",
    "print(flowData.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     time     value  nhan\n",
      "0 1900-01-01 12:37:05.162  0.148586     0\n",
      "1 1900-01-01 12:37:05.167  0.148548     0\n",
      "2 1900-01-01 12:37:05.172  0.148485     0\n",
      "3 1900-01-01 12:37:05.177  0.148397     0\n",
      "4 1900-01-01 12:37:05.182  0.148284     0\n"
     ]
    }
   ],
   "source": [
    "for index, row in datatimeOA.iterrows():\n",
    "    mask = (flowData['time'] >= row['StartDate']) & (flowData['time'] <= row['endDate'])\n",
    "    flowData.loc[mask, 'nhan'] = 1\n",
    "    \n",
    "\n",
    "print(flowData.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           time     value  nhan\n",
      "72168   1900-01-01 12:43:06.002 -0.065932     1\n",
      "72169   1900-01-01 12:43:06.007 -0.065745     1\n",
      "72170   1900-01-01 12:43:06.012 -0.065540     1\n",
      "72171   1900-01-01 12:43:06.017 -0.065319     1\n",
      "72172   1900-01-01 12:43:06.022 -0.065085     1\n",
      "...                         ...       ...   ...\n",
      "1048570 1900-01-01 02:04:28.012  0.196029     1\n",
      "1048571 1900-01-01 02:04:28.017  0.203181     1\n",
      "1048572 1900-01-01 02:04:28.022  0.210161     1\n",
      "1048573 1900-01-01 02:04:28.027  0.216970     1\n",
      "1048574 1900-01-01 02:04:28.032  0.223612     1\n",
      "\n",
      "[598923 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "flowData_nhan_1 = flowData[flowData['nhan'] == 1]\n",
    "print(flowData_nhan_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def create_training_data(data, window_size, step_size):\n",
    "\n",
    "    _train = []\n",
    "    _label = []\n",
    "\n",
    "    for i in range(0, data.shape[0] - window_size, step_size):\n",
    "        try:\n",
    "            xs = data['value'].values[i: i + window_size]\n",
    "        \n",
    "            # Skip examples where the activity label changes within the window.\n",
    "            if (data['nhan'][i+1] != data['nhan'][i + window_size-1]):\n",
    "                print(f\"Skipping index {i} due to label change within window.\")\n",
    "                continue\n",
    "\n",
    "            label = data['nhan'][i + window_size-1]\n",
    "\n",
    "            # Skip examples where the label is NaN.\n",
    "            if math.isnan(label):\n",
    "                print(f\"Skipping index {i} due to NaN label.\")\n",
    "                continue\n",
    "\n",
    "            _train.append(xs)\n",
    "            _label.append(label)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred at index {i}: {e}\")\n",
    "\n",
    "    return _train, _label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping index 69000 due to label change within window.\n",
      "Skipping index 70000 due to label change within window.\n",
      "Skipping index 71000 due to label change within window.\n",
      "Skipping index 73000 due to label change within window.\n",
      "Skipping index 74000 due to label change within window.\n",
      "Skipping index 75000 due to label change within window.\n",
      "Skipping index 90000 due to label change within window.\n",
      "Skipping index 91000 due to label change within window.\n",
      "Skipping index 92000 due to label change within window.\n",
      "Skipping index 94000 due to label change within window.\n",
      "Skipping index 95000 due to label change within window.\n",
      "Skipping index 96000 due to label change within window.\n",
      "Skipping index 109000 due to label change within window.\n",
      "Skipping index 110000 due to label change within window.\n",
      "Skipping index 112000 due to label change within window.\n",
      "Skipping index 114000 due to label change within window.\n",
      "Skipping index 115000 due to label change within window.\n",
      "Skipping index 118000 due to label change within window.\n",
      "Skipping index 121000 due to label change within window.\n",
      "Skipping index 122000 due to label change within window.\n",
      "Skipping index 123000 due to label change within window.\n",
      "Skipping index 124000 due to label change within window.\n",
      "Skipping index 125000 due to label change within window.\n",
      "Skipping index 126000 due to label change within window.\n",
      "Skipping index 127000 due to label change within window.\n",
      "Skipping index 128000 due to label change within window.\n",
      "Skipping index 130000 due to label change within window.\n",
      "Skipping index 133000 due to label change within window.\n",
      "Skipping index 134000 due to label change within window.\n",
      "Skipping index 137000 due to label change within window.\n",
      "Skipping index 138000 due to label change within window.\n",
      "Skipping index 139000 due to label change within window.\n",
      "Skipping index 143000 due to label change within window.\n",
      "Skipping index 146000 due to label change within window.\n",
      "Skipping index 148000 due to label change within window.\n",
      "Skipping index 151000 due to label change within window.\n",
      "Skipping index 155000 due to label change within window.\n",
      "Skipping index 164000 due to label change within window.\n",
      "Skipping index 167000 due to label change within window.\n",
      "Skipping index 176000 due to label change within window.\n",
      "Skipping index 179000 due to label change within window.\n",
      "Skipping index 181000 due to label change within window.\n",
      "Skipping index 182000 due to label change within window.\n",
      "Skipping index 183000 due to label change within window.\n",
      "Skipping index 184000 due to label change within window.\n",
      "Skipping index 185000 due to label change within window.\n",
      "Skipping index 186000 due to label change within window.\n",
      "Skipping index 187000 due to label change within window.\n",
      "Skipping index 197000 due to label change within window.\n",
      "Skipping index 213000 due to label change within window.\n",
      "Skipping index 216000 due to label change within window.\n",
      "Skipping index 217000 due to label change within window.\n",
      "Skipping index 220000 due to label change within window.\n",
      "Skipping index 229000 due to label change within window.\n",
      "Skipping index 230000 due to label change within window.\n",
      "Skipping index 231000 due to label change within window.\n",
      "Skipping index 232000 due to label change within window.\n",
      "Skipping index 233000 due to label change within window.\n",
      "Skipping index 235000 due to label change within window.\n",
      "Skipping index 236000 due to label change within window.\n",
      "Skipping index 237000 due to label change within window.\n",
      "Skipping index 238000 due to label change within window.\n",
      "Skipping index 239000 due to label change within window.\n",
      "Skipping index 242000 due to label change within window.\n",
      "Skipping index 246000 due to label change within window.\n",
      "Skipping index 247000 due to label change within window.\n",
      "Skipping index 252000 due to label change within window.\n",
      "Skipping index 256000 due to label change within window.\n",
      "Skipping index 261000 due to label change within window.\n",
      "Skipping index 265000 due to label change within window.\n",
      "Skipping index 266000 due to label change within window.\n",
      "Skipping index 267000 due to label change within window.\n",
      "Skipping index 270000 due to label change within window.\n",
      "Skipping index 276000 due to label change within window.\n",
      "Skipping index 283000 due to label change within window.\n",
      "Skipping index 288000 due to label change within window.\n",
      "Skipping index 289000 due to label change within window.\n",
      "Skipping index 290000 due to label change within window.\n",
      "Skipping index 291000 due to label change within window.\n",
      "Skipping index 292000 due to label change within window.\n",
      "Skipping index 293000 due to label change within window.\n",
      "Skipping index 294000 due to label change within window.\n",
      "Skipping index 295000 due to label change within window.\n",
      "Skipping index 305000 due to label change within window.\n",
      "Skipping index 308000 due to label change within window.\n",
      "Skipping index 314000 due to label change within window.\n",
      "Skipping index 317000 due to label change within window.\n",
      "Skipping index 319000 due to label change within window.\n",
      "Skipping index 322000 due to label change within window.\n",
      "Skipping index 327000 due to label change within window.\n",
      "Skipping index 328000 due to label change within window.\n",
      "Skipping index 329000 due to label change within window.\n",
      "Skipping index 332000 due to label change within window.\n",
      "Skipping index 337000 due to label change within window.\n",
      "Skipping index 338000 due to label change within window.\n",
      "Skipping index 342000 due to label change within window.\n",
      "Skipping index 343000 due to label change within window.\n",
      "Skipping index 347000 due to label change within window.\n",
      "Skipping index 348000 due to label change within window.\n",
      "Skipping index 352000 due to label change within window.\n",
      "Skipping index 353000 due to label change within window.\n",
      "Skipping index 354000 due to label change within window.\n",
      "Skipping index 355000 due to label change within window.\n",
      "Skipping index 356000 due to label change within window.\n",
      "Skipping index 357000 due to label change within window.\n",
      "Skipping index 358000 due to label change within window.\n",
      "Skipping index 359000 due to label change within window.\n",
      "Skipping index 364000 due to label change within window.\n",
      "Skipping index 365000 due to label change within window.\n",
      "Skipping index 368000 due to label change within window.\n",
      "Skipping index 373000 due to label change within window.\n",
      "Skipping index 380000 due to label change within window.\n",
      "Skipping index 381000 due to label change within window.\n",
      "Skipping index 382000 due to label change within window.\n",
      "Skipping index 385000 due to label change within window.\n",
      "Skipping index 386000 due to label change within window.\n",
      "Skipping index 387000 due to label change within window.\n",
      "Skipping index 388000 due to label change within window.\n",
      "Skipping index 391000 due to label change within window.\n",
      "Skipping index 392000 due to label change within window.\n",
      "Skipping index 396000 due to label change within window.\n",
      "Skipping index 397000 due to label change within window.\n",
      "Skipping index 400000 due to label change within window.\n",
      "Skipping index 401000 due to label change within window.\n",
      "Skipping index 406000 due to label change within window.\n",
      "Skipping index 407000 due to label change within window.\n",
      "Skipping index 410000 due to label change within window.\n",
      "Skipping index 412000 due to label change within window.\n",
      "Skipping index 413000 due to label change within window.\n",
      "Skipping index 414000 due to label change within window.\n",
      "Skipping index 416000 due to label change within window.\n",
      "Skipping index 417000 due to label change within window.\n",
      "Skipping index 418000 due to label change within window.\n",
      "Skipping index 419000 due to label change within window.\n",
      "Skipping index 420000 due to label change within window.\n",
      "Skipping index 421000 due to label change within window.\n",
      "Skipping index 422000 due to label change within window.\n",
      "Skipping index 424000 due to label change within window.\n",
      "Skipping index 425000 due to label change within window.\n",
      "Skipping index 427000 due to label change within window.\n",
      "Skipping index 431000 due to label change within window.\n",
      "Skipping index 434000 due to label change within window.\n",
      "Skipping index 435000 due to label change within window.\n",
      "Skipping index 440000 due to label change within window.\n",
      "Skipping index 444000 due to label change within window.\n",
      "Skipping index 448000 due to label change within window.\n",
      "Skipping index 452000 due to label change within window.\n",
      "Skipping index 456000 due to label change within window.\n",
      "Skipping index 463000 due to label change within window.\n",
      "Skipping index 465000 due to label change within window.\n",
      "Skipping index 468000 due to label change within window.\n",
      "Skipping index 473000 due to label change within window.\n",
      "Skipping index 482000 due to label change within window.\n",
      "Skipping index 486000 due to label change within window.\n",
      "Skipping index 487000 due to label change within window.\n",
      "Skipping index 488000 due to label change within window.\n",
      "Skipping index 491000 due to label change within window.\n",
      "Skipping index 492000 due to label change within window.\n",
      "Skipping index 493000 due to label change within window.\n",
      "Skipping index 494000 due to label change within window.\n",
      "Skipping index 497000 due to label change within window.\n",
      "Skipping index 502000 due to label change within window.\n",
      "Skipping index 506000 due to label change within window.\n",
      "Skipping index 507000 due to label change within window.\n",
      "Skipping index 510000 due to label change within window.\n",
      "Skipping index 512000 due to label change within window.\n",
      "Skipping index 515000 due to label change within window.\n",
      "Skipping index 516000 due to label change within window.\n",
      "Skipping index 517000 due to label change within window.\n",
      "Skipping index 521000 due to label change within window.\n",
      "Skipping index 522000 due to label change within window.\n",
      "Skipping index 526000 due to label change within window.\n",
      "Skipping index 527000 due to label change within window.\n",
      "Skipping index 528000 due to label change within window.\n",
      "Skipping index 531000 due to label change within window.\n",
      "Skipping index 532000 due to label change within window.\n",
      "Skipping index 533000 due to label change within window.\n",
      "Skipping index 534000 due to label change within window.\n",
      "Skipping index 537000 due to label change within window.\n",
      "Skipping index 539000 due to label change within window.\n",
      "Skipping index 542000 due to label change within window.\n",
      "Skipping index 547000 due to label change within window.\n",
      "Skipping index 549000 due to label change within window.\n",
      "Skipping index 553000 due to label change within window.\n",
      "Skipping index 554000 due to label change within window.\n",
      "Skipping index 558000 due to label change within window.\n",
      "Skipping index 559000 due to label change within window.\n",
      "Skipping index 560000 due to label change within window.\n",
      "Skipping index 563000 due to label change within window.\n",
      "Skipping index 565000 due to label change within window.\n",
      "Skipping index 568000 due to label change within window.\n",
      "Skipping index 569000 due to label change within window.\n",
      "Skipping index 570000 due to label change within window.\n",
      "Skipping index 574000 due to label change within window.\n",
      "Skipping index 575000 due to label change within window.\n",
      "Skipping index 580000 due to label change within window.\n",
      "Skipping index 581000 due to label change within window.\n",
      "Skipping index 583000 due to label change within window.\n",
      "Skipping index 584000 due to label change within window.\n",
      "Skipping index 585000 due to label change within window.\n",
      "Skipping index 586000 due to label change within window.\n",
      "Skipping index 587000 due to label change within window.\n",
      "Skipping index 588000 due to label change within window.\n",
      "Skipping index 589000 due to label change within window.\n",
      "Skipping index 590000 due to label change within window.\n",
      "Skipping index 591000 due to label change within window.\n",
      "Skipping index 592000 due to label change within window.\n",
      "Skipping index 602000 due to label change within window.\n",
      "Skipping index 603000 due to label change within window.\n",
      "Skipping index 604000 due to label change within window.\n",
      "Skipping index 608000 due to label change within window.\n",
      "Skipping index 609000 due to label change within window.\n",
      "Skipping index 610000 due to label change within window.\n",
      "Skipping index 615000 due to label change within window.\n",
      "Skipping index 616000 due to label change within window.\n",
      "Skipping index 621000 due to label change within window.\n",
      "Skipping index 624000 due to label change within window.\n",
      "Skipping index 626000 due to label change within window.\n",
      "Skipping index 627000 due to label change within window.\n",
      "Skipping index 628000 due to label change within window.\n",
      "Skipping index 631000 due to label change within window.\n",
      "Skipping index 632000 due to label change within window.\n",
      "Skipping index 634000 due to label change within window.\n",
      "Skipping index 642000 due to label change within window.\n",
      "Skipping index 645000 due to label change within window.\n",
      "Skipping index 651000 due to label change within window.\n",
      "Skipping index 652000 due to label change within window.\n",
      "Skipping index 653000 due to label change within window.\n",
      "Skipping index 659000 due to label change within window.\n",
      "Skipping index 660000 due to label change within window.\n",
      "Skipping index 665000 due to label change within window.\n",
      "Skipping index 671000 due to label change within window.\n",
      "Skipping index 672000 due to label change within window.\n",
      "Skipping index 674000 due to label change within window.\n",
      "Skipping index 675000 due to label change within window.\n",
      "Skipping index 680000 due to label change within window.\n",
      "Skipping index 681000 due to label change within window.\n",
      "Skipping index 685000 due to label change within window.\n",
      "Skipping index 686000 due to label change within window.\n",
      "Skipping index 694000 due to label change within window.\n",
      "Skipping index 695000 due to label change within window.\n",
      "Skipping index 696000 due to label change within window.\n",
      "Skipping index 697000 due to label change within window.\n",
      "Skipping index 699000 due to label change within window.\n",
      "Skipping index 700000 due to label change within window.\n",
      "Skipping index 701000 due to label change within window.\n",
      "Skipping index 702000 due to label change within window.\n",
      "Skipping index 710000 due to label change within window.\n",
      "Skipping index 711000 due to label change within window.\n",
      "Skipping index 712000 due to label change within window.\n",
      "Skipping index 713000 due to label change within window.\n",
      "Skipping index 715000 due to label change within window.\n",
      "Skipping index 716000 due to label change within window.\n",
      "Skipping index 717000 due to label change within window.\n",
      "Skipping index 721000 due to label change within window.\n",
      "Skipping index 722000 due to label change within window.\n",
      "Skipping index 725000 due to label change within window.\n",
      "Skipping index 726000 due to label change within window.\n",
      "Skipping index 727000 due to label change within window.\n",
      "Skipping index 731000 due to label change within window.\n",
      "Skipping index 732000 due to label change within window.\n",
      "Skipping index 737000 due to label change within window.\n",
      "Skipping index 740000 due to label change within window.\n",
      "Skipping index 741000 due to label change within window.\n",
      "Skipping index 742000 due to label change within window.\n",
      "Skipping index 743000 due to label change within window.\n",
      "Skipping index 744000 due to label change within window.\n",
      "Skipping index 746000 due to label change within window.\n",
      "Skipping index 747000 due to label change within window.\n",
      "Skipping index 748000 due to label change within window.\n",
      "Skipping index 749000 due to label change within window.\n",
      "Skipping index 750000 due to label change within window.\n",
      "Skipping index 753000 due to label change within window.\n",
      "Skipping index 754000 due to label change within window.\n",
      "Skipping index 760000 due to label change within window.\n",
      "Skipping index 761000 due to label change within window.\n",
      "Skipping index 764000 due to label change within window.\n",
      "Skipping index 765000 due to label change within window.\n",
      "Skipping index 767000 due to label change within window.\n",
      "Skipping index 771000 due to label change within window.\n",
      "Skipping index 773000 due to label change within window.\n",
      "Skipping index 774000 due to label change within window.\n",
      "Skipping index 775000 due to label change within window.\n",
      "Skipping index 777000 due to label change within window.\n",
      "Skipping index 778000 due to label change within window.\n",
      "Skipping index 779000 due to label change within window.\n",
      "Skipping index 780000 due to label change within window.\n",
      "Skipping index 781000 due to label change within window.\n",
      "Skipping index 784000 due to label change within window.\n",
      "Skipping index 785000 due to label change within window.\n",
      "Skipping index 791000 due to label change within window.\n",
      "Skipping index 795000 due to label change within window.\n",
      "Skipping index 796000 due to label change within window.\n",
      "Skipping index 797000 due to label change within window.\n",
      "Skipping index 800000 due to label change within window.\n",
      "Skipping index 801000 due to label change within window.\n",
      "Skipping index 804000 due to label change within window.\n",
      "Skipping index 805000 due to label change within window.\n",
      "Skipping index 806000 due to label change within window.\n",
      "Skipping index 807000 due to label change within window.\n",
      "Skipping index 831000 due to label change within window.\n",
      "Skipping index 832000 due to label change within window.\n",
      "Skipping index 833000 due to label change within window.\n",
      "Skipping index 834000 due to label change within window.\n",
      "Skipping index 835000 due to label change within window.\n",
      "Skipping index 836000 due to label change within window.\n",
      "Skipping index 839000 due to label change within window.\n",
      "Skipping index 841000 due to label change within window.\n",
      "Skipping index 844000 due to label change within window.\n",
      "Skipping index 845000 due to label change within window.\n",
      "Skipping index 846000 due to label change within window.\n",
      "Skipping index 847000 due to label change within window.\n",
      "Skipping index 850000 due to label change within window.\n",
      "Skipping index 851000 due to label change within window.\n",
      "Skipping index 852000 due to label change within window.\n",
      "Skipping index 853000 due to label change within window.\n",
      "Skipping index 856000 due to label change within window.\n",
      "Skipping index 857000 due to label change within window.\n",
      "Skipping index 858000 due to label change within window.\n",
      "Skipping index 863000 due to label change within window.\n",
      "Skipping index 866000 due to label change within window.\n",
      "Skipping index 868000 due to label change within window.\n",
      "Skipping index 869000 due to label change within window.\n",
      "Skipping index 870000 due to label change within window.\n",
      "Skipping index 873000 due to label change within window.\n",
      "Skipping index 874000 due to label change within window.\n",
      "Skipping index 875000 due to label change within window.\n",
      "Skipping index 877000 due to label change within window.\n",
      "Skipping index 880000 due to label change within window.\n",
      "Skipping index 881000 due to label change within window.\n",
      "Skipping index 882000 due to label change within window.\n",
      "Skipping index 886000 due to label change within window.\n",
      "Skipping index 887000 due to label change within window.\n",
      "Skipping index 888000 due to label change within window.\n",
      "Skipping index 889000 due to label change within window.\n",
      "Skipping index 891000 due to label change within window.\n",
      "Skipping index 894000 due to label change within window.\n",
      "Skipping index 896000 due to label change within window.\n",
      "Skipping index 897000 due to label change within window.\n",
      "Skipping index 902000 due to label change within window.\n",
      "Skipping index 903000 due to label change within window.\n",
      "Skipping index 904000 due to label change within window.\n",
      "Skipping index 907000 due to label change within window.\n",
      "Skipping index 908000 due to label change within window.\n",
      "Skipping index 910000 due to label change within window.\n",
      "Skipping index 913000 due to label change within window.\n",
      "Skipping index 914000 due to label change within window.\n",
      "Skipping index 915000 due to label change within window.\n",
      "Skipping index 916000 due to label change within window.\n",
      "Skipping index 919000 due to label change within window.\n",
      "Skipping index 920000 due to label change within window.\n",
      "Skipping index 922000 due to label change within window.\n",
      "Skipping index 923000 due to label change within window.\n",
      "Skipping index 926000 due to label change within window.\n",
      "Skipping index 927000 due to label change within window.\n",
      "Skipping index 929000 due to label change within window.\n",
      "Skipping index 933000 due to label change within window.\n",
      "Skipping index 934000 due to label change within window.\n",
      "Skipping index 935000 due to label change within window.\n",
      "Skipping index 938000 due to label change within window.\n",
      "Skipping index 940000 due to label change within window.\n",
      "Skipping index 941000 due to label change within window.\n",
      "Skipping index 942000 due to label change within window.\n",
      "Skipping index 961000 due to label change within window.\n",
      "Skipping index 962000 due to label change within window.\n",
      "Skipping index 963000 due to label change within window.\n",
      "Skipping index 964000 due to label change within window.\n",
      "Skipping index 967000 due to label change within window.\n",
      "Skipping index 968000 due to label change within window.\n",
      "Skipping index 971000 due to label change within window.\n",
      "Skipping index 973000 due to label change within window.\n",
      "Skipping index 974000 due to label change within window.\n",
      "Skipping index 975000 due to label change within window.\n",
      "Skipping index 976000 due to label change within window.\n",
      "Skipping index 977000 due to label change within window.\n",
      "Skipping index 978000 due to label change within window.\n",
      "Skipping index 980000 due to label change within window.\n",
      "Skipping index 983000 due to label change within window.\n",
      "Skipping index 988000 due to label change within window.\n",
      "Skipping index 989000 due to label change within window.\n",
      "Skipping index 990000 due to label change within window.\n",
      "Skipping index 993000 due to label change within window.\n",
      "Skipping index 994000 due to label change within window.\n",
      "Skipping index 995000 due to label change within window.\n",
      "Skipping index 996000 due to label change within window.\n",
      "Skipping index 999000 due to label change within window.\n",
      "Skipping index 1000000 due to label change within window.\n",
      "Skipping index 1001000 due to label change within window.\n",
      "Skipping index 1002000 due to label change within window.\n",
      "Skipping index 1005000 due to label change within window.\n",
      "Skipping index 1006000 due to label change within window.\n",
      "Skipping index 1008000 due to label change within window.\n",
      "Skipping index 1012000 due to label change within window.\n",
      "Skipping index 1013000 due to label change within window.\n",
      "Skipping index 1017000 due to label change within window.\n",
      "Skipping index 1018000 due to label change within window.\n",
      "Skipping index 1022000 due to label change within window.\n",
      "Skipping index 1024000 due to label change within window.\n",
      "Skipping index 1025000 due to label change within window.\n",
      "Skipping index 1026000 due to label change within window.\n",
      "Skipping index 1027000 due to label change within window.\n",
      "Skipping index 1028000 due to label change within window.\n",
      "Skipping index 1029000 due to label change within window.\n",
      "Skipping index 1030000 due to label change within window.\n",
      "Skipping index 1031000 due to label change within window.\n",
      "Skipping index 1033000 due to label change within window.\n",
      "Skipping index 1037000 due to label change within window.\n",
      "Skipping index 1038000 due to label change within window.\n",
      "Skipping index 1039000 due to label change within window.\n",
      "Skipping index 1042000 due to label change within window.\n",
      "Skipping index 1043000 due to label change within window.\n",
      "Skipping index 1044000 due to label change within window.\n"
     ]
    }
   ],
   "source": [
    "_train, _label = create_training_data(\n",
    "    data=flowData, window_size=4000, step_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x1ac0e5978e0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "audio_data = _train[0]\n",
    "\n",
    "audio_data = np.array(audio_data).astype(\"float\")\n",
    "sgram = librosa.stft(audio_data)\n",
    "# librosa.display.specshow(sgram)\n",
    "sgram_mag, _  = librosa.magphase(sgram)\n",
    "mel_scale_sgram = librosa.feature.melspectrogram(S=sgram_mag, sr =4000)\n",
    "print(mel_scale_sgram.shape)\n",
    "# librosa.display.specshow(mel_scale_sgram)\n",
    "mel_sgram = librosa.amplitude_to_db(mel_scale_sgram , ref=np.min)\n",
    "librosa.display.specshow(mel_sgram ,sr =200, x_axis=\"time\", y_axis=\"mel\")\n",
    "plt.colorbar(format='%+0.2f dB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import torch\n",
    "import tensorflow\n",
    "def calcuMelSgram(signal):\n",
    "  sgram = librosa.stft(signal)  \n",
    "  sgram_mag, _  = librosa.magphase(sgram)\n",
    "  mel_scale_sgram = librosa.feature.melspectrogram(S=sgram_mag, sr = 4000)\n",
    "  mel_sgram = librosa.amplitude_to_db(mel_scale_sgram , ref=np.min)\n",
    "  # t = torch.Tensor(mel_sgram)\n",
    "  # t = tensorflow.convert_to_tensor(mel_sgram, dtype=tensorflow.float32)\n",
    "  return mel_sgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMFE = []\n",
    "for i in range(len(_train)):\n",
    "  audio_data = np.array(_train[i]).astype(\"float\")\n",
    "  trainMFE.append(calcuMelSgram(audio_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(trainMFE, _label, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 8)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 128, 8)\n",
      "(127, 128, 8)\n",
      "(506, 2)\n",
      "(127, 2)\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "x_train_array = np.array(x_train.tolist())\n",
    "\n",
    "x_train_reshaped = x_train_array.reshape(x_train_array.shape[0], 128, 8, 1)\n",
    "\n",
    "x_train_reshaped = x_train_reshaped.astype('float32')\n",
    "\n",
    "\n",
    "x_test_array = np.array(x_test.tolist())\n",
    "\n",
    "x_test_reshaped = x_test_array.reshape(x_test_array.shape[0], 128, 8, 1)\n",
    "\n",
    "x_test_reshaped = x_test_reshaped.astype('float32')\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_encoded = to_categorical(y_train)\n",
    "y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "print(x_train_array.shape)\n",
    "print(x_test_array.shape)\n",
    "print(y_train_encoded.shape)\n",
    "print(y_test_encoded.shape)\n",
    "print(y_train_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_16 (Conv2D)          (None, 128, 8, 8)         208       \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPooli  (None, 64, 4, 8)          0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 64, 4, 8)          0         \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 64, 4, 16)         1168      \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPooli  (None, 32, 2, 16)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 32, 2, 16)         0         \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               262400    \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 264290 (1.01 MB)\n",
      "Trainable params: 264290 (1.01 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "16/16 [==============================] - 1s 15ms/step - loss: 8.9237 - accuracy: 0.5573 - val_loss: 3.8580 - val_accuracy: 0.6142\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.5759 - accuracy: 0.5455 - val_loss: 1.0493 - val_accuracy: 0.6142\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.8350 - accuracy: 0.6225 - val_loss: 0.4205 - val_accuracy: 0.8189\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.0980 - accuracy: 0.6265 - val_loss: 0.6466 - val_accuracy: 0.6299\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.8289 - accuracy: 0.6581 - val_loss: 0.5363 - val_accuracy: 0.6693\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.7020 - accuracy: 0.7095 - val_loss: 0.4898 - val_accuracy: 0.7717\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5878 - accuracy: 0.7490 - val_loss: 0.4767 - val_accuracy: 0.8268\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5448 - accuracy: 0.7668 - val_loss: 0.4681 - val_accuracy: 0.8268\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5289 - accuracy: 0.7767 - val_loss: 0.4552 - val_accuracy: 0.8268\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5396 - accuracy: 0.7648 - val_loss: 0.4578 - val_accuracy: 0.8268\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5271 - accuracy: 0.7747 - val_loss: 0.4648 - val_accuracy: 0.8268\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4912 - accuracy: 0.7945 - val_loss: 0.4606 - val_accuracy: 0.8189\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4855 - accuracy: 0.7905 - val_loss: 0.4553 - val_accuracy: 0.8110\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4792 - accuracy: 0.7846 - val_loss: 0.4449 - val_accuracy: 0.7953\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4823 - accuracy: 0.8043 - val_loss: 0.4480 - val_accuracy: 0.8268\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4833 - accuracy: 0.8024 - val_loss: 0.4386 - val_accuracy: 0.8189\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4699 - accuracy: 0.7945 - val_loss: 0.4337 - val_accuracy: 0.8189\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4733 - accuracy: 0.7747 - val_loss: 0.4325 - val_accuracy: 0.8268\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4469 - accuracy: 0.8063 - val_loss: 0.4292 - val_accuracy: 0.8189\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4714 - accuracy: 0.8083 - val_loss: 0.4158 - val_accuracy: 0.8346\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4470 - accuracy: 0.8063 - val_loss: 0.4151 - val_accuracy: 0.8268\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4554 - accuracy: 0.8043 - val_loss: 0.4146 - val_accuracy: 0.8268\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4263 - accuracy: 0.8281 - val_loss: 0.4040 - val_accuracy: 0.8425\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4442 - accuracy: 0.8399 - val_loss: 0.4085 - val_accuracy: 0.8268\n",
      "Epoch 25/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4430 - accuracy: 0.8142 - val_loss: 0.4043 - val_accuracy: 0.8425\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4104 - accuracy: 0.8577 - val_loss: 0.4059 - val_accuracy: 0.8189\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4541 - accuracy: 0.8142 - val_loss: 0.4057 - val_accuracy: 0.8268\n",
      "Epoch 28/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4129 - accuracy: 0.8221 - val_loss: 0.4036 - val_accuracy: 0.8346\n",
      "Epoch 29/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4236 - accuracy: 0.8043 - val_loss: 0.3977 - val_accuracy: 0.8346\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4177 - accuracy: 0.8221 - val_loss: 0.3933 - val_accuracy: 0.8268\n",
      "Epoch 31/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4270 - accuracy: 0.8241 - val_loss: 0.3934 - val_accuracy: 0.8189\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4384 - accuracy: 0.8083 - val_loss: 0.3936 - val_accuracy: 0.8268\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4320 - accuracy: 0.8162 - val_loss: 0.3988 - val_accuracy: 0.8268\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4269 - accuracy: 0.8182 - val_loss: 0.3946 - val_accuracy: 0.8189\n",
      "Epoch 35/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4183 - accuracy: 0.8261 - val_loss: 0.3893 - val_accuracy: 0.8268\n",
      "Epoch 36/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4168 - accuracy: 0.8221 - val_loss: 0.3775 - val_accuracy: 0.8425\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4231 - accuracy: 0.8162 - val_loss: 0.3729 - val_accuracy: 0.8425\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4005 - accuracy: 0.8300 - val_loss: 0.3683 - val_accuracy: 0.8425\n",
      "Epoch 39/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4303 - accuracy: 0.8142 - val_loss: 0.3685 - val_accuracy: 0.8504\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3914 - accuracy: 0.8498 - val_loss: 0.3677 - val_accuracy: 0.8583\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4145 - accuracy: 0.8458 - val_loss: 0.3721 - val_accuracy: 0.8346\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3989 - accuracy: 0.8241 - val_loss: 0.3646 - val_accuracy: 0.8504\n",
      "Epoch 43/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3997 - accuracy: 0.8221 - val_loss: 0.3677 - val_accuracy: 0.8425\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3671 - accuracy: 0.8577 - val_loss: 0.3577 - val_accuracy: 0.8425\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4042 - accuracy: 0.8419 - val_loss: 0.3647 - val_accuracy: 0.8425\n",
      "Epoch 46/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3747 - accuracy: 0.8518 - val_loss: 0.3576 - val_accuracy: 0.8661\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4003 - accuracy: 0.8202 - val_loss: 0.3636 - val_accuracy: 0.8425\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4185 - accuracy: 0.8399 - val_loss: 0.3522 - val_accuracy: 0.8583\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3848 - accuracy: 0.8478 - val_loss: 0.3599 - val_accuracy: 0.8268\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3982 - accuracy: 0.8379 - val_loss: 0.3501 - val_accuracy: 0.8504\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4106 - accuracy: 0.8439 - val_loss: 0.3483 - val_accuracy: 0.8583\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3833 - accuracy: 0.8518 - val_loss: 0.3372 - val_accuracy: 0.8425\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3953 - accuracy: 0.8379 - val_loss: 0.3492 - val_accuracy: 0.8504\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3770 - accuracy: 0.8478 - val_loss: 0.3376 - val_accuracy: 0.8583\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3769 - accuracy: 0.8597 - val_loss: 0.3348 - val_accuracy: 0.8819\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3668 - accuracy: 0.8439 - val_loss: 0.3321 - val_accuracy: 0.8583\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4176 - accuracy: 0.8300 - val_loss: 0.3357 - val_accuracy: 0.8583\n",
      "Epoch 58/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3610 - accuracy: 0.8518 - val_loss: 0.3335 - val_accuracy: 0.8661\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3973 - accuracy: 0.8399 - val_loss: 0.3297 - val_accuracy: 0.8661\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3839 - accuracy: 0.8340 - val_loss: 0.3279 - val_accuracy: 0.8740\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3648 - accuracy: 0.8656 - val_loss: 0.3201 - val_accuracy: 0.8504\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3856 - accuracy: 0.8439 - val_loss: 0.3263 - val_accuracy: 0.8661\n",
      "Epoch 63/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3650 - accuracy: 0.8458 - val_loss: 0.3307 - val_accuracy: 0.8819\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3532 - accuracy: 0.8636 - val_loss: 0.3279 - val_accuracy: 0.8504\n",
      "Epoch 65/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4002 - accuracy: 0.8458 - val_loss: 0.3336 - val_accuracy: 0.8661\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3565 - accuracy: 0.8577 - val_loss: 0.3222 - val_accuracy: 0.8583\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3619 - accuracy: 0.8419 - val_loss: 0.3219 - val_accuracy: 0.8740\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3834 - accuracy: 0.8498 - val_loss: 0.3312 - val_accuracy: 0.8740\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3730 - accuracy: 0.8617 - val_loss: 0.3350 - val_accuracy: 0.8661\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3952 - accuracy: 0.8557 - val_loss: 0.3260 - val_accuracy: 0.8819\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3459 - accuracy: 0.8518 - val_loss: 0.3283 - val_accuracy: 0.8583\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3654 - accuracy: 0.8439 - val_loss: 0.3201 - val_accuracy: 0.8661\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3375 - accuracy: 0.8656 - val_loss: 0.3184 - val_accuracy: 0.8661\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3572 - accuracy: 0.8557 - val_loss: 0.3123 - val_accuracy: 0.8740\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3430 - accuracy: 0.8636 - val_loss: 0.3185 - val_accuracy: 0.8661\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3522 - accuracy: 0.8557 - val_loss: 0.3149 - val_accuracy: 0.8583\n",
      "Epoch 77/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3821 - accuracy: 0.8478 - val_loss: 0.3202 - val_accuracy: 0.8583\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3564 - accuracy: 0.8518 - val_loss: 0.3096 - val_accuracy: 0.8740\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3360 - accuracy: 0.8478 - val_loss: 0.3045 - val_accuracy: 0.8740\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3290 - accuracy: 0.8696 - val_loss: 0.2991 - val_accuracy: 0.8583\n",
      "Epoch 81/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3560 - accuracy: 0.8557 - val_loss: 0.3098 - val_accuracy: 0.8819\n",
      "Epoch 82/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3405 - accuracy: 0.8617 - val_loss: 0.3050 - val_accuracy: 0.8819\n",
      "Epoch 83/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3143 - accuracy: 0.8755 - val_loss: 0.3049 - val_accuracy: 0.8740\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3550 - accuracy: 0.8557 - val_loss: 0.3059 - val_accuracy: 0.8661\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3474 - accuracy: 0.8715 - val_loss: 0.3072 - val_accuracy: 0.8740\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3332 - accuracy: 0.8538 - val_loss: 0.2956 - val_accuracy: 0.8661\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3693 - accuracy: 0.8518 - val_loss: 0.3398 - val_accuracy: 0.8189\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3486 - accuracy: 0.8498 - val_loss: 0.3057 - val_accuracy: 0.8819\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3397 - accuracy: 0.8735 - val_loss: 0.2953 - val_accuracy: 0.8740\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3139 - accuracy: 0.8854 - val_loss: 0.2926 - val_accuracy: 0.8740\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3412 - accuracy: 0.8696 - val_loss: 0.2902 - val_accuracy: 0.8819\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3436 - accuracy: 0.8458 - val_loss: 0.2912 - val_accuracy: 0.8976\n",
      "Epoch 93/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3714 - accuracy: 0.8478 - val_loss: 0.3118 - val_accuracy: 0.8740\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3432 - accuracy: 0.8715 - val_loss: 0.3032 - val_accuracy: 0.8661\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3310 - accuracy: 0.8755 - val_loss: 0.2993 - val_accuracy: 0.8819\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3398 - accuracy: 0.8656 - val_loss: 0.2885 - val_accuracy: 0.8819\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3313 - accuracy: 0.8597 - val_loss: 0.2963 - val_accuracy: 0.8898\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3582 - accuracy: 0.8577 - val_loss: 0.2846 - val_accuracy: 0.8898\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3223 - accuracy: 0.8814 - val_loss: 0.2864 - val_accuracy: 0.8898\n",
      "Epoch 100/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3464 - accuracy: 0.8617 - val_loss: 0.2908 - val_accuracy: 0.8740\n",
      "Epoch 101/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3266 - accuracy: 0.8597 - val_loss: 0.2843 - val_accuracy: 0.8819\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3275 - accuracy: 0.8636 - val_loss: 0.2914 - val_accuracy: 0.8898\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3400 - accuracy: 0.8676 - val_loss: 0.2875 - val_accuracy: 0.8819\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3324 - accuracy: 0.8676 - val_loss: 0.2828 - val_accuracy: 0.8898\n",
      "Epoch 105/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3261 - accuracy: 0.8814 - val_loss: 0.2817 - val_accuracy: 0.8976\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3230 - accuracy: 0.8715 - val_loss: 0.2878 - val_accuracy: 0.8976\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3218 - accuracy: 0.8656 - val_loss: 0.2882 - val_accuracy: 0.8976\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3082 - accuracy: 0.8577 - val_loss: 0.2803 - val_accuracy: 0.8898\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3068 - accuracy: 0.8656 - val_loss: 0.2781 - val_accuracy: 0.8898\n",
      "Epoch 110/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3181 - accuracy: 0.8597 - val_loss: 0.2730 - val_accuracy: 0.9055\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3067 - accuracy: 0.8696 - val_loss: 0.2730 - val_accuracy: 0.8976\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3085 - accuracy: 0.8676 - val_loss: 0.2751 - val_accuracy: 0.9134\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3188 - accuracy: 0.8735 - val_loss: 0.2892 - val_accuracy: 0.8898\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3125 - accuracy: 0.8794 - val_loss: 0.2780 - val_accuracy: 0.8976\n",
      "Epoch 115/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3244 - accuracy: 0.8814 - val_loss: 0.2799 - val_accuracy: 0.8898\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2865 - accuracy: 0.8834 - val_loss: 0.2714 - val_accuracy: 0.8898\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3278 - accuracy: 0.8775 - val_loss: 0.2741 - val_accuracy: 0.9055\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2939 - accuracy: 0.8834 - val_loss: 0.2799 - val_accuracy: 0.8976\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3044 - accuracy: 0.8775 - val_loss: 0.2823 - val_accuracy: 0.8819\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3045 - accuracy: 0.8775 - val_loss: 0.2721 - val_accuracy: 0.8976\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2916 - accuracy: 0.8893 - val_loss: 0.2909 - val_accuracy: 0.8661\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3646 - accuracy: 0.8538 - val_loss: 0.2653 - val_accuracy: 0.9055\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3401 - accuracy: 0.8656 - val_loss: 0.2795 - val_accuracy: 0.8976\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2963 - accuracy: 0.8794 - val_loss: 0.2818 - val_accuracy: 0.8976\n",
      "Epoch 125/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3344 - accuracy: 0.8538 - val_loss: 0.2794 - val_accuracy: 0.8898\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2981 - accuracy: 0.8834 - val_loss: 0.2876 - val_accuracy: 0.8819\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3371 - accuracy: 0.8696 - val_loss: 0.2742 - val_accuracy: 0.9055\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2990 - accuracy: 0.8755 - val_loss: 0.2858 - val_accuracy: 0.8819\n",
      "Epoch 129/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3099 - accuracy: 0.8874 - val_loss: 0.2680 - val_accuracy: 0.9055\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2899 - accuracy: 0.8636 - val_loss: 0.2765 - val_accuracy: 0.8976\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2959 - accuracy: 0.8696 - val_loss: 0.2756 - val_accuracy: 0.9055\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3233 - accuracy: 0.8617 - val_loss: 0.3029 - val_accuracy: 0.8583\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3054 - accuracy: 0.8715 - val_loss: 0.2783 - val_accuracy: 0.8819\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2941 - accuracy: 0.8794 - val_loss: 0.3008 - val_accuracy: 0.8661\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2988 - accuracy: 0.8814 - val_loss: 0.2722 - val_accuracy: 0.9055\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2912 - accuracy: 0.8715 - val_loss: 0.2705 - val_accuracy: 0.9055\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3052 - accuracy: 0.8755 - val_loss: 0.2845 - val_accuracy: 0.8740\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3074 - accuracy: 0.8874 - val_loss: 0.2669 - val_accuracy: 0.8976\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3007 - accuracy: 0.8656 - val_loss: 0.2719 - val_accuracy: 0.8819\n",
      "Epoch 140/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3049 - accuracy: 0.8775 - val_loss: 0.2821 - val_accuracy: 0.8740\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3014 - accuracy: 0.8735 - val_loss: 0.2657 - val_accuracy: 0.9055\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2808 - accuracy: 0.8893 - val_loss: 0.2614 - val_accuracy: 0.8898\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3112 - accuracy: 0.8656 - val_loss: 0.2576 - val_accuracy: 0.9055\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2822 - accuracy: 0.8755 - val_loss: 0.2742 - val_accuracy: 0.8898\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3081 - accuracy: 0.8794 - val_loss: 0.2779 - val_accuracy: 0.8740\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2916 - accuracy: 0.8676 - val_loss: 0.2796 - val_accuracy: 0.8976\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3064 - accuracy: 0.8775 - val_loss: 0.2853 - val_accuracy: 0.8740\n",
      "Epoch 148/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2976 - accuracy: 0.8834 - val_loss: 0.2881 - val_accuracy: 0.8583\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3157 - accuracy: 0.8676 - val_loss: 0.2750 - val_accuracy: 0.9055\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2744 - accuracy: 0.8933 - val_loss: 0.2707 - val_accuracy: 0.8976\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2884 - accuracy: 0.8755 - val_loss: 0.2951 - val_accuracy: 0.8740\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3046 - accuracy: 0.8874 - val_loss: 0.2653 - val_accuracy: 0.9055\n",
      "Epoch 153/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3099 - accuracy: 0.8834 - val_loss: 0.2803 - val_accuracy: 0.9055\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2947 - accuracy: 0.8735 - val_loss: 0.2696 - val_accuracy: 0.8976\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2945 - accuracy: 0.8893 - val_loss: 0.2841 - val_accuracy: 0.8976\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3014 - accuracy: 0.8794 - val_loss: 0.2811 - val_accuracy: 0.8898\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2899 - accuracy: 0.8893 - val_loss: 0.2916 - val_accuracy: 0.8740\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2982 - accuracy: 0.8676 - val_loss: 0.2710 - val_accuracy: 0.8976\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3033 - accuracy: 0.8794 - val_loss: 0.2664 - val_accuracy: 0.8976\n",
      "Epoch 160/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2978 - accuracy: 0.8755 - val_loss: 0.2822 - val_accuracy: 0.8740\n",
      "Epoch 161/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2868 - accuracy: 0.8696 - val_loss: 0.2713 - val_accuracy: 0.8976\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2859 - accuracy: 0.8933 - val_loss: 0.2983 - val_accuracy: 0.8661\n",
      "Epoch 163/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2862 - accuracy: 0.8834 - val_loss: 0.2589 - val_accuracy: 0.9055\n",
      "Epoch 164/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2948 - accuracy: 0.8933 - val_loss: 0.2896 - val_accuracy: 0.8819\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2881 - accuracy: 0.8874 - val_loss: 0.2700 - val_accuracy: 0.8976\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2810 - accuracy: 0.9012 - val_loss: 0.3176 - val_accuracy: 0.8661\n",
      "Epoch 167/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3007 - accuracy: 0.8854 - val_loss: 0.2688 - val_accuracy: 0.8976\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2818 - accuracy: 0.8715 - val_loss: 0.3082 - val_accuracy: 0.8898\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2844 - accuracy: 0.8814 - val_loss: 0.2655 - val_accuracy: 0.9055\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2796 - accuracy: 0.8893 - val_loss: 0.2678 - val_accuracy: 0.8898\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3078 - accuracy: 0.8814 - val_loss: 0.2799 - val_accuracy: 0.8819\n",
      "Epoch 172/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2787 - accuracy: 0.8893 - val_loss: 0.2598 - val_accuracy: 0.9055\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2964 - accuracy: 0.8933 - val_loss: 0.2738 - val_accuracy: 0.8819\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2830 - accuracy: 0.8953 - val_loss: 0.3047 - val_accuracy: 0.8661\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2805 - accuracy: 0.8656 - val_loss: 0.2692 - val_accuracy: 0.8976\n",
      "Epoch 176/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2635 - accuracy: 0.8854 - val_loss: 0.2890 - val_accuracy: 0.8819\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2842 - accuracy: 0.8874 - val_loss: 0.2767 - val_accuracy: 0.8819\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2774 - accuracy: 0.8735 - val_loss: 0.2725 - val_accuracy: 0.9134\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2699 - accuracy: 0.8874 - val_loss: 0.2799 - val_accuracy: 0.9055\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2800 - accuracy: 0.8874 - val_loss: 0.2744 - val_accuracy: 0.8740\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2526 - accuracy: 0.9032 - val_loss: 0.2739 - val_accuracy: 0.8898\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2940 - accuracy: 0.8913 - val_loss: 0.2663 - val_accuracy: 0.8819\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2818 - accuracy: 0.8854 - val_loss: 0.2883 - val_accuracy: 0.8819\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2806 - accuracy: 0.8913 - val_loss: 0.2629 - val_accuracy: 0.8898\n",
      "Epoch 185/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2604 - accuracy: 0.8913 - val_loss: 0.2596 - val_accuracy: 0.8819\n",
      "Epoch 186/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2586 - accuracy: 0.8933 - val_loss: 0.2614 - val_accuracy: 0.8976\n",
      "Epoch 187/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2701 - accuracy: 0.8834 - val_loss: 0.2572 - val_accuracy: 0.8819\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2895 - accuracy: 0.8834 - val_loss: 0.2611 - val_accuracy: 0.8976\n",
      "Epoch 189/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2920 - accuracy: 0.8854 - val_loss: 0.2865 - val_accuracy: 0.8740\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2806 - accuracy: 0.8854 - val_loss: 0.3263 - val_accuracy: 0.8504\n",
      "Epoch 191/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2666 - accuracy: 0.8992 - val_loss: 0.2625 - val_accuracy: 0.9055\n",
      "Epoch 192/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2811 - accuracy: 0.8834 - val_loss: 0.2765 - val_accuracy: 0.8819\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2794 - accuracy: 0.8933 - val_loss: 0.2759 - val_accuracy: 0.8661\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2683 - accuracy: 0.8913 - val_loss: 0.2826 - val_accuracy: 0.8819\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2618 - accuracy: 0.8953 - val_loss: 0.2691 - val_accuracy: 0.8661\n",
      "Epoch 196/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2453 - accuracy: 0.8874 - val_loss: 0.2895 - val_accuracy: 0.8583\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2623 - accuracy: 0.8913 - val_loss: 0.2715 - val_accuracy: 0.8976\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2741 - accuracy: 0.8893 - val_loss: 0.3008 - val_accuracy: 0.8583\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2558 - accuracy: 0.8992 - val_loss: 0.2642 - val_accuracy: 0.8898\n",
      "Epoch 200/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2766 - accuracy: 0.8755 - val_loss: 0.2936 - val_accuracy: 0.8819\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.2884 - accuracy: 0.8814 - val_loss: 0.2580 - val_accuracy: 0.8976\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2616 - accuracy: 0.8992 - val_loss: 0.3440 - val_accuracy: 0.8740\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2833 - accuracy: 0.8953 - val_loss: 0.2575 - val_accuracy: 0.9055\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2835 - accuracy: 0.9012 - val_loss: 0.2657 - val_accuracy: 0.8898\n",
      "Epoch 205/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3043 - accuracy: 0.8755 - val_loss: 0.3126 - val_accuracy: 0.8583\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2871 - accuracy: 0.8893 - val_loss: 0.2797 - val_accuracy: 0.8898\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2765 - accuracy: 0.8893 - val_loss: 0.2756 - val_accuracy: 0.8976\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2673 - accuracy: 0.8953 - val_loss: 0.2730 - val_accuracy: 0.8740\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2542 - accuracy: 0.8972 - val_loss: 0.2877 - val_accuracy: 0.8898\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2625 - accuracy: 0.8933 - val_loss: 0.2841 - val_accuracy: 0.8661\n",
      "Epoch 211/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2654 - accuracy: 0.8834 - val_loss: 0.2588 - val_accuracy: 0.9055\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2651 - accuracy: 0.8775 - val_loss: 0.2688 - val_accuracy: 0.9055\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2531 - accuracy: 0.8992 - val_loss: 0.2700 - val_accuracy: 0.9055\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2779 - accuracy: 0.8893 - val_loss: 0.2877 - val_accuracy: 0.8819\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2776 - accuracy: 0.8913 - val_loss: 0.2687 - val_accuracy: 0.8976\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2683 - accuracy: 0.8854 - val_loss: 0.2762 - val_accuracy: 0.8898\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2486 - accuracy: 0.8972 - val_loss: 0.2717 - val_accuracy: 0.9134\n",
      "Epoch 218/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2613 - accuracy: 0.8913 - val_loss: 0.2833 - val_accuracy: 0.8583\n",
      "Epoch 219/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2553 - accuracy: 0.8933 - val_loss: 0.2624 - val_accuracy: 0.8898\n",
      "Epoch 220/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2609 - accuracy: 0.8953 - val_loss: 0.2659 - val_accuracy: 0.9055\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2784 - accuracy: 0.8814 - val_loss: 0.2738 - val_accuracy: 0.8898\n",
      "Epoch 222/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2833 - accuracy: 0.8814 - val_loss: 0.2868 - val_accuracy: 0.8898\n",
      "Epoch 223/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2856 - accuracy: 0.8676 - val_loss: 0.2692 - val_accuracy: 0.8740\n",
      "Epoch 224/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2486 - accuracy: 0.8972 - val_loss: 0.2845 - val_accuracy: 0.8898\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2422 - accuracy: 0.8992 - val_loss: 0.2666 - val_accuracy: 0.8819\n",
      "Epoch 226/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2492 - accuracy: 0.8953 - val_loss: 0.2798 - val_accuracy: 0.8898\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2539 - accuracy: 0.8874 - val_loss: 0.2710 - val_accuracy: 0.9055\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2556 - accuracy: 0.9051 - val_loss: 0.2735 - val_accuracy: 0.8898\n",
      "Epoch 229/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2468 - accuracy: 0.8854 - val_loss: 0.2871 - val_accuracy: 0.8819\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2547 - accuracy: 0.9071 - val_loss: 0.2677 - val_accuracy: 0.8740\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2684 - accuracy: 0.8953 - val_loss: 0.3235 - val_accuracy: 0.8661\n",
      "Epoch 232/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2598 - accuracy: 0.8972 - val_loss: 0.2864 - val_accuracy: 0.8583\n",
      "Epoch 233/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2654 - accuracy: 0.8913 - val_loss: 0.3281 - val_accuracy: 0.8661\n",
      "Epoch 234/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2677 - accuracy: 0.8854 - val_loss: 0.2812 - val_accuracy: 0.8740\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2625 - accuracy: 0.8972 - val_loss: 0.3309 - val_accuracy: 0.8661\n",
      "Epoch 236/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2360 - accuracy: 0.9051 - val_loss: 0.3028 - val_accuracy: 0.8740\n",
      "Epoch 237/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2489 - accuracy: 0.8834 - val_loss: 0.2765 - val_accuracy: 0.8976\n",
      "Epoch 238/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2247 - accuracy: 0.9190 - val_loss: 0.2872 - val_accuracy: 0.8819\n",
      "Epoch 239/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2516 - accuracy: 0.8933 - val_loss: 0.2979 - val_accuracy: 0.8976\n",
      "Epoch 240/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2341 - accuracy: 0.9051 - val_loss: 0.2746 - val_accuracy: 0.8898\n",
      "Epoch 241/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2351 - accuracy: 0.9051 - val_loss: 0.2768 - val_accuracy: 0.8661\n",
      "Epoch 242/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2566 - accuracy: 0.8933 - val_loss: 0.2952 - val_accuracy: 0.8740\n",
      "Epoch 243/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2791 - accuracy: 0.8893 - val_loss: 0.2999 - val_accuracy: 0.8740\n",
      "Epoch 244/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2524 - accuracy: 0.8992 - val_loss: 0.3113 - val_accuracy: 0.8661\n",
      "Epoch 245/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2617 - accuracy: 0.8775 - val_loss: 0.2664 - val_accuracy: 0.8976\n",
      "Epoch 246/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2504 - accuracy: 0.8953 - val_loss: 0.2738 - val_accuracy: 0.8976\n",
      "Epoch 247/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2340 - accuracy: 0.8874 - val_loss: 0.2830 - val_accuracy: 0.9055\n",
      "Epoch 248/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2542 - accuracy: 0.8992 - val_loss: 0.3128 - val_accuracy: 0.8898\n",
      "Epoch 249/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2626 - accuracy: 0.8913 - val_loss: 0.2706 - val_accuracy: 0.9055\n",
      "Epoch 250/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2605 - accuracy: 0.8874 - val_loss: 0.2611 - val_accuracy: 0.8898\n",
      "Epoch 251/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.2206 - accuracy: 0.9091 - val_loss: 0.2784 - val_accuracy: 0.8976\n",
      "Epoch 252/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2393 - accuracy: 0.9111 - val_loss: 0.2898 - val_accuracy: 0.8819\n",
      "Epoch 253/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2360 - accuracy: 0.9012 - val_loss: 0.3186 - val_accuracy: 0.8819\n",
      "Epoch 254/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2144 - accuracy: 0.9229 - val_loss: 0.2852 - val_accuracy: 0.8819\n",
      "Epoch 255/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2359 - accuracy: 0.8953 - val_loss: 0.2812 - val_accuracy: 0.8819\n",
      "Epoch 256/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2346 - accuracy: 0.9091 - val_loss: 0.2786 - val_accuracy: 0.8661\n",
      "Epoch 257/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2507 - accuracy: 0.8874 - val_loss: 0.2919 - val_accuracy: 0.8898\n",
      "Epoch 258/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2590 - accuracy: 0.8834 - val_loss: 0.3038 - val_accuracy: 0.8976\n",
      "Epoch 259/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2614 - accuracy: 0.8874 - val_loss: 0.2703 - val_accuracy: 0.9055\n",
      "Epoch 260/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2305 - accuracy: 0.9071 - val_loss: 0.2813 - val_accuracy: 0.9055\n",
      "Epoch 261/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2319 - accuracy: 0.9170 - val_loss: 0.2933 - val_accuracy: 0.8976\n",
      "Epoch 262/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2509 - accuracy: 0.9012 - val_loss: 0.2890 - val_accuracy: 0.8976\n",
      "Epoch 263/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2357 - accuracy: 0.9032 - val_loss: 0.3029 - val_accuracy: 0.8898\n",
      "Epoch 264/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2415 - accuracy: 0.8893 - val_loss: 0.2845 - val_accuracy: 0.8898\n",
      "Epoch 265/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2606 - accuracy: 0.8893 - val_loss: 0.3014 - val_accuracy: 0.8740\n",
      "Epoch 266/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2399 - accuracy: 0.9091 - val_loss: 0.2884 - val_accuracy: 0.8583\n",
      "Epoch 267/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2485 - accuracy: 0.9130 - val_loss: 0.2958 - val_accuracy: 0.8819\n",
      "Epoch 268/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2760 - accuracy: 0.8834 - val_loss: 0.2838 - val_accuracy: 0.8976\n",
      "Epoch 269/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2449 - accuracy: 0.8992 - val_loss: 0.2798 - val_accuracy: 0.9055\n",
      "Epoch 270/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2360 - accuracy: 0.8972 - val_loss: 0.2725 - val_accuracy: 0.9134\n",
      "Epoch 271/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2400 - accuracy: 0.9170 - val_loss: 0.2760 - val_accuracy: 0.9134\n",
      "Epoch 272/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2195 - accuracy: 0.9071 - val_loss: 0.2673 - val_accuracy: 0.9213\n",
      "Epoch 273/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2365 - accuracy: 0.9012 - val_loss: 0.2845 - val_accuracy: 0.8898\n",
      "Epoch 274/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2282 - accuracy: 0.9012 - val_loss: 0.2839 - val_accuracy: 0.8898\n",
      "Epoch 275/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2228 - accuracy: 0.9071 - val_loss: 0.2973 - val_accuracy: 0.8976\n",
      "Epoch 276/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2298 - accuracy: 0.9111 - val_loss: 0.2892 - val_accuracy: 0.9055\n",
      "Epoch 277/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2270 - accuracy: 0.9091 - val_loss: 0.2677 - val_accuracy: 0.9134\n",
      "Epoch 278/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2458 - accuracy: 0.9012 - val_loss: 0.2909 - val_accuracy: 0.8976\n",
      "Epoch 279/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2305 - accuracy: 0.9012 - val_loss: 0.2926 - val_accuracy: 0.9055\n",
      "Epoch 280/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2495 - accuracy: 0.9051 - val_loss: 0.2780 - val_accuracy: 0.8976\n",
      "Epoch 281/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2321 - accuracy: 0.9071 - val_loss: 0.2783 - val_accuracy: 0.8976\n",
      "Epoch 282/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2246 - accuracy: 0.9249 - val_loss: 0.2831 - val_accuracy: 0.9055\n",
      "Epoch 283/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2212 - accuracy: 0.9170 - val_loss: 0.2757 - val_accuracy: 0.8976\n",
      "Epoch 284/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2203 - accuracy: 0.9150 - val_loss: 0.3012 - val_accuracy: 0.8898\n",
      "Epoch 285/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2309 - accuracy: 0.9051 - val_loss: 0.2776 - val_accuracy: 0.9055\n",
      "Epoch 286/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2391 - accuracy: 0.9091 - val_loss: 0.2720 - val_accuracy: 0.9055\n",
      "Epoch 287/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2492 - accuracy: 0.8972 - val_loss: 0.2584 - val_accuracy: 0.8976\n",
      "Epoch 288/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2345 - accuracy: 0.8893 - val_loss: 0.2838 - val_accuracy: 0.8976\n",
      "Epoch 289/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2230 - accuracy: 0.9051 - val_loss: 0.2862 - val_accuracy: 0.8976\n",
      "Epoch 290/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2287 - accuracy: 0.9150 - val_loss: 0.2604 - val_accuracy: 0.8976\n",
      "Epoch 291/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2590 - accuracy: 0.8874 - val_loss: 0.2608 - val_accuracy: 0.9055\n",
      "Epoch 292/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2365 - accuracy: 0.9012 - val_loss: 0.2677 - val_accuracy: 0.9055\n",
      "Epoch 293/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2360 - accuracy: 0.9051 - val_loss: 0.2905 - val_accuracy: 0.8819\n",
      "Epoch 294/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.2338 - accuracy: 0.8913 - val_loss: 0.3086 - val_accuracy: 0.8740\n",
      "Epoch 295/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2287 - accuracy: 0.8972 - val_loss: 0.2727 - val_accuracy: 0.8898\n",
      "Epoch 296/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2297 - accuracy: 0.8992 - val_loss: 0.2637 - val_accuracy: 0.9213\n",
      "Epoch 297/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2421 - accuracy: 0.8992 - val_loss: 0.2646 - val_accuracy: 0.9055\n",
      "Epoch 298/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2253 - accuracy: 0.9032 - val_loss: 0.2708 - val_accuracy: 0.9213\n",
      "Epoch 299/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2250 - accuracy: 0.9130 - val_loss: 0.2680 - val_accuracy: 0.9055\n",
      "Epoch 300/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2446 - accuracy: 0.8972 - val_loss: 0.2927 - val_accuracy: 0.9055\n",
      "Epoch 301/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2164 - accuracy: 0.8972 - val_loss: 0.2844 - val_accuracy: 0.9134\n",
      "Epoch 302/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2253 - accuracy: 0.8992 - val_loss: 0.2848 - val_accuracy: 0.8898\n",
      "Epoch 303/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2125 - accuracy: 0.9170 - val_loss: 0.2912 - val_accuracy: 0.8819\n",
      "Epoch 304/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2252 - accuracy: 0.9032 - val_loss: 0.2638 - val_accuracy: 0.8898\n",
      "Epoch 305/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2173 - accuracy: 0.9051 - val_loss: 0.2813 - val_accuracy: 0.8976\n",
      "Epoch 306/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2332 - accuracy: 0.9051 - val_loss: 0.2587 - val_accuracy: 0.9213\n",
      "Epoch 307/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2455 - accuracy: 0.8893 - val_loss: 0.3034 - val_accuracy: 0.8740\n",
      "Epoch 308/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2555 - accuracy: 0.8814 - val_loss: 0.2803 - val_accuracy: 0.8976\n",
      "Epoch 309/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2257 - accuracy: 0.9071 - val_loss: 0.2942 - val_accuracy: 0.8976\n",
      "Epoch 310/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2314 - accuracy: 0.9071 - val_loss: 0.2813 - val_accuracy: 0.8898\n",
      "Epoch 311/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2304 - accuracy: 0.9091 - val_loss: 0.2786 - val_accuracy: 0.8976\n",
      "Epoch 312/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2272 - accuracy: 0.9150 - val_loss: 0.2691 - val_accuracy: 0.9134\n",
      "Epoch 313/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2453 - accuracy: 0.9051 - val_loss: 0.2624 - val_accuracy: 0.9055\n",
      "Epoch 314/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2322 - accuracy: 0.8972 - val_loss: 0.2916 - val_accuracy: 0.8976\n",
      "Epoch 315/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2305 - accuracy: 0.9091 - val_loss: 0.2891 - val_accuracy: 0.8976\n",
      "Epoch 316/500\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.2141 - accuracy: 0.9091 - val_loss: 0.2680 - val_accuracy: 0.9055\n",
      "Epoch 317/500\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.2148 - accuracy: 0.9150 - val_loss: 0.2728 - val_accuracy: 0.9134\n",
      "Epoch 318/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2254 - accuracy: 0.9190 - val_loss: 0.2705 - val_accuracy: 0.8976\n",
      "Epoch 319/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2456 - accuracy: 0.8972 - val_loss: 0.3030 - val_accuracy: 0.8976\n",
      "Epoch 320/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2259 - accuracy: 0.9091 - val_loss: 0.2573 - val_accuracy: 0.9055\n",
      "Epoch 321/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2072 - accuracy: 0.9111 - val_loss: 0.2949 - val_accuracy: 0.8898\n",
      "Epoch 322/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2334 - accuracy: 0.9091 - val_loss: 0.2718 - val_accuracy: 0.9134\n",
      "Epoch 323/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2360 - accuracy: 0.9150 - val_loss: 0.2878 - val_accuracy: 0.9055\n",
      "Epoch 324/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.2132 - accuracy: 0.9150 - val_loss: 0.2695 - val_accuracy: 0.9134\n",
      "Epoch 325/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2074 - accuracy: 0.9071 - val_loss: 0.2752 - val_accuracy: 0.8898\n",
      "Epoch 326/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2056 - accuracy: 0.9190 - val_loss: 0.2714 - val_accuracy: 0.8976\n",
      "Epoch 327/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2202 - accuracy: 0.9091 - val_loss: 0.2702 - val_accuracy: 0.8976\n",
      "Epoch 328/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2473 - accuracy: 0.9032 - val_loss: 0.2791 - val_accuracy: 0.9134\n",
      "Epoch 329/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2199 - accuracy: 0.9130 - val_loss: 0.2953 - val_accuracy: 0.8740\n",
      "Epoch 330/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2155 - accuracy: 0.9051 - val_loss: 0.2909 - val_accuracy: 0.8898\n",
      "Epoch 331/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2374 - accuracy: 0.9012 - val_loss: 0.2876 - val_accuracy: 0.9055\n",
      "Epoch 332/500\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.2211 - accuracy: 0.9150 - val_loss: 0.2756 - val_accuracy: 0.9134\n",
      "Epoch 333/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2193 - accuracy: 0.9071 - val_loss: 0.2857 - val_accuracy: 0.8976\n",
      "Epoch 334/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2017 - accuracy: 0.9249 - val_loss: 0.2796 - val_accuracy: 0.8819\n",
      "Epoch 335/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2223 - accuracy: 0.9150 - val_loss: 0.2717 - val_accuracy: 0.9213\n",
      "Epoch 336/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2102 - accuracy: 0.9071 - val_loss: 0.2671 - val_accuracy: 0.9291\n",
      "Epoch 337/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2064 - accuracy: 0.9111 - val_loss: 0.2954 - val_accuracy: 0.9055\n",
      "Epoch 338/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2062 - accuracy: 0.9170 - val_loss: 0.2867 - val_accuracy: 0.8976\n",
      "Epoch 339/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2398 - accuracy: 0.9091 - val_loss: 0.2787 - val_accuracy: 0.8976\n",
      "Epoch 340/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2294 - accuracy: 0.9051 - val_loss: 0.2657 - val_accuracy: 0.9134\n",
      "Epoch 341/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2204 - accuracy: 0.9150 - val_loss: 0.3019 - val_accuracy: 0.8976\n",
      "Epoch 342/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2043 - accuracy: 0.9229 - val_loss: 0.2760 - val_accuracy: 0.9134\n",
      "Epoch 343/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2320 - accuracy: 0.9032 - val_loss: 0.2965 - val_accuracy: 0.9134\n",
      "Epoch 344/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2206 - accuracy: 0.9111 - val_loss: 0.2728 - val_accuracy: 0.9134\n",
      "Epoch 345/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2251 - accuracy: 0.8953 - val_loss: 0.2944 - val_accuracy: 0.8976\n",
      "Epoch 346/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2305 - accuracy: 0.9051 - val_loss: 0.2862 - val_accuracy: 0.9055\n",
      "Epoch 347/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2227 - accuracy: 0.9032 - val_loss: 0.2794 - val_accuracy: 0.9134\n",
      "Epoch 348/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2244 - accuracy: 0.9012 - val_loss: 0.2944 - val_accuracy: 0.8976\n",
      "Epoch 349/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2009 - accuracy: 0.9229 - val_loss: 0.3114 - val_accuracy: 0.8898\n",
      "Epoch 350/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2042 - accuracy: 0.9130 - val_loss: 0.2960 - val_accuracy: 0.8819\n",
      "Epoch 351/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1915 - accuracy: 0.9249 - val_loss: 0.2868 - val_accuracy: 0.8976\n",
      "Epoch 352/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2401 - accuracy: 0.8972 - val_loss: 0.2831 - val_accuracy: 0.9055\n",
      "Epoch 353/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2366 - accuracy: 0.9032 - val_loss: 0.2774 - val_accuracy: 0.9134\n",
      "Epoch 354/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2470 - accuracy: 0.9032 - val_loss: 0.2838 - val_accuracy: 0.9055\n",
      "Epoch 355/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2753 - accuracy: 0.8933 - val_loss: 0.2992 - val_accuracy: 0.8740\n",
      "Epoch 356/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2303 - accuracy: 0.9071 - val_loss: 0.2737 - val_accuracy: 0.8976\n",
      "Epoch 357/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2054 - accuracy: 0.9111 - val_loss: 0.3010 - val_accuracy: 0.8976\n",
      "Epoch 358/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1962 - accuracy: 0.9190 - val_loss: 0.2852 - val_accuracy: 0.9134\n",
      "Epoch 359/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2233 - accuracy: 0.9249 - val_loss: 0.3075 - val_accuracy: 0.8819\n",
      "Epoch 360/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2117 - accuracy: 0.9071 - val_loss: 0.2782 - val_accuracy: 0.9134\n",
      "Epoch 361/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2066 - accuracy: 0.9190 - val_loss: 0.2773 - val_accuracy: 0.9055\n",
      "Epoch 362/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2068 - accuracy: 0.9150 - val_loss: 0.2857 - val_accuracy: 0.9055\n",
      "Epoch 363/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2236 - accuracy: 0.9091 - val_loss: 0.2934 - val_accuracy: 0.9055\n",
      "Epoch 364/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2211 - accuracy: 0.9111 - val_loss: 0.2984 - val_accuracy: 0.9134\n",
      "Epoch 365/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2058 - accuracy: 0.9150 - val_loss: 0.2864 - val_accuracy: 0.9055\n",
      "Epoch 366/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.1903 - accuracy: 0.9249 - val_loss: 0.2829 - val_accuracy: 0.9134\n",
      "Epoch 367/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2155 - accuracy: 0.9229 - val_loss: 0.2978 - val_accuracy: 0.8898\n",
      "Epoch 368/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.2089 - accuracy: 0.9368 - val_loss: 0.2824 - val_accuracy: 0.9055\n",
      "Epoch 369/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2186 - accuracy: 0.9150 - val_loss: 0.2789 - val_accuracy: 0.8898\n",
      "Epoch 370/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2020 - accuracy: 0.9091 - val_loss: 0.2960 - val_accuracy: 0.8976\n",
      "Epoch 371/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1955 - accuracy: 0.9130 - val_loss: 0.2871 - val_accuracy: 0.8976\n",
      "Epoch 372/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1999 - accuracy: 0.9229 - val_loss: 0.2782 - val_accuracy: 0.9055\n",
      "Epoch 373/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2061 - accuracy: 0.9150 - val_loss: 0.2833 - val_accuracy: 0.9213\n",
      "Epoch 374/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1932 - accuracy: 0.9111 - val_loss: 0.3068 - val_accuracy: 0.9134\n",
      "Epoch 375/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2127 - accuracy: 0.9229 - val_loss: 0.2846 - val_accuracy: 0.8976\n",
      "Epoch 376/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2056 - accuracy: 0.9091 - val_loss: 0.3050 - val_accuracy: 0.9055\n",
      "Epoch 377/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1970 - accuracy: 0.9190 - val_loss: 0.3091 - val_accuracy: 0.8898\n",
      "Epoch 378/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2026 - accuracy: 0.9269 - val_loss: 0.3242 - val_accuracy: 0.8898\n",
      "Epoch 379/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2094 - accuracy: 0.9091 - val_loss: 0.2942 - val_accuracy: 0.9134\n",
      "Epoch 380/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2002 - accuracy: 0.9229 - val_loss: 0.3041 - val_accuracy: 0.9213\n",
      "Epoch 381/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1836 - accuracy: 0.9190 - val_loss: 0.2894 - val_accuracy: 0.9055\n",
      "Epoch 382/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1880 - accuracy: 0.9209 - val_loss: 0.2962 - val_accuracy: 0.8898\n",
      "Epoch 383/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1967 - accuracy: 0.9130 - val_loss: 0.2900 - val_accuracy: 0.8898\n",
      "Epoch 384/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1940 - accuracy: 0.9130 - val_loss: 0.3308 - val_accuracy: 0.8819\n",
      "Epoch 385/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.2072 - accuracy: 0.9209 - val_loss: 0.3005 - val_accuracy: 0.9055\n",
      "Epoch 386/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1879 - accuracy: 0.9229 - val_loss: 0.3102 - val_accuracy: 0.9055\n",
      "Epoch 387/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2086 - accuracy: 0.9091 - val_loss: 0.2870 - val_accuracy: 0.8976\n",
      "Epoch 388/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2393 - accuracy: 0.9012 - val_loss: 0.3049 - val_accuracy: 0.8976\n",
      "Epoch 389/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2043 - accuracy: 0.9170 - val_loss: 0.3049 - val_accuracy: 0.8976\n",
      "Epoch 390/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1989 - accuracy: 0.9150 - val_loss: 0.2989 - val_accuracy: 0.9055\n",
      "Epoch 391/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1829 - accuracy: 0.9229 - val_loss: 0.2829 - val_accuracy: 0.9134\n",
      "Epoch 392/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1734 - accuracy: 0.9269 - val_loss: 0.3340 - val_accuracy: 0.8898\n",
      "Epoch 393/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2069 - accuracy: 0.9289 - val_loss: 0.2842 - val_accuracy: 0.8976\n",
      "Epoch 394/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1902 - accuracy: 0.9091 - val_loss: 0.3527 - val_accuracy: 0.8740\n",
      "Epoch 395/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1918 - accuracy: 0.9170 - val_loss: 0.2934 - val_accuracy: 0.8898\n",
      "Epoch 396/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2178 - accuracy: 0.9150 - val_loss: 0.2979 - val_accuracy: 0.9213\n",
      "Epoch 397/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1967 - accuracy: 0.9249 - val_loss: 0.2970 - val_accuracy: 0.9134\n",
      "Epoch 398/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1888 - accuracy: 0.9150 - val_loss: 0.2946 - val_accuracy: 0.9213\n",
      "Epoch 399/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2203 - accuracy: 0.9091 - val_loss: 0.2844 - val_accuracy: 0.9134\n",
      "Epoch 400/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2252 - accuracy: 0.8992 - val_loss: 0.2971 - val_accuracy: 0.9055\n",
      "Epoch 401/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2178 - accuracy: 0.9091 - val_loss: 0.2951 - val_accuracy: 0.9055\n",
      "Epoch 402/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2039 - accuracy: 0.9170 - val_loss: 0.2999 - val_accuracy: 0.9134\n",
      "Epoch 403/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1942 - accuracy: 0.9269 - val_loss: 0.2887 - val_accuracy: 0.9055\n",
      "Epoch 404/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1758 - accuracy: 0.9229 - val_loss: 0.2817 - val_accuracy: 0.9213\n",
      "Epoch 405/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1889 - accuracy: 0.9249 - val_loss: 0.2994 - val_accuracy: 0.9213\n",
      "Epoch 406/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2158 - accuracy: 0.9111 - val_loss: 0.2659 - val_accuracy: 0.9055\n",
      "Epoch 407/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2117 - accuracy: 0.9091 - val_loss: 0.2941 - val_accuracy: 0.9134\n",
      "Epoch 408/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.1964 - accuracy: 0.9111 - val_loss: 0.3309 - val_accuracy: 0.8898\n",
      "Epoch 409/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1918 - accuracy: 0.9111 - val_loss: 0.2919 - val_accuracy: 0.9055\n",
      "Epoch 410/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1829 - accuracy: 0.9269 - val_loss: 0.3050 - val_accuracy: 0.9055\n",
      "Epoch 411/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1884 - accuracy: 0.9249 - val_loss: 0.2787 - val_accuracy: 0.9055\n",
      "Epoch 412/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.1793 - accuracy: 0.9249 - val_loss: 0.2854 - val_accuracy: 0.9055\n",
      "Epoch 413/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2051 - accuracy: 0.9170 - val_loss: 0.2981 - val_accuracy: 0.8976\n",
      "Epoch 414/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1918 - accuracy: 0.9289 - val_loss: 0.2999 - val_accuracy: 0.8898\n",
      "Epoch 415/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2032 - accuracy: 0.9308 - val_loss: 0.2863 - val_accuracy: 0.9055\n",
      "Epoch 416/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1849 - accuracy: 0.9190 - val_loss: 0.2752 - val_accuracy: 0.9134\n",
      "Epoch 417/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.2078 - accuracy: 0.9032 - val_loss: 0.3354 - val_accuracy: 0.9055\n",
      "Epoch 418/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2129 - accuracy: 0.8992 - val_loss: 0.2914 - val_accuracy: 0.9134\n",
      "Epoch 419/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1902 - accuracy: 0.9190 - val_loss: 0.2897 - val_accuracy: 0.9134\n",
      "Epoch 420/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.1817 - accuracy: 0.9269 - val_loss: 0.2776 - val_accuracy: 0.9134\n",
      "Epoch 421/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2031 - accuracy: 0.9308 - val_loss: 0.2987 - val_accuracy: 0.9134\n",
      "Epoch 422/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1876 - accuracy: 0.9269 - val_loss: 0.2838 - val_accuracy: 0.9055\n",
      "Epoch 423/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1743 - accuracy: 0.9229 - val_loss: 0.3116 - val_accuracy: 0.9055\n",
      "Epoch 424/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1952 - accuracy: 0.9091 - val_loss: 0.2882 - val_accuracy: 0.9055\n",
      "Epoch 425/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1706 - accuracy: 0.9348 - val_loss: 0.2840 - val_accuracy: 0.9134\n",
      "Epoch 426/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1606 - accuracy: 0.9368 - val_loss: 0.2554 - val_accuracy: 0.9134\n",
      "Epoch 427/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1791 - accuracy: 0.9150 - val_loss: 0.2891 - val_accuracy: 0.8898\n",
      "Epoch 428/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1741 - accuracy: 0.9190 - val_loss: 0.2810 - val_accuracy: 0.9055\n",
      "Epoch 429/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1649 - accuracy: 0.9308 - val_loss: 0.3056 - val_accuracy: 0.9055\n",
      "Epoch 430/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1864 - accuracy: 0.9150 - val_loss: 0.2760 - val_accuracy: 0.9213\n",
      "Epoch 431/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1954 - accuracy: 0.9170 - val_loss: 0.3135 - val_accuracy: 0.8976\n",
      "Epoch 432/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.1940 - accuracy: 0.9249 - val_loss: 0.2948 - val_accuracy: 0.9055\n",
      "Epoch 433/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1704 - accuracy: 0.9289 - val_loss: 0.3064 - val_accuracy: 0.8819\n",
      "Epoch 434/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1842 - accuracy: 0.9308 - val_loss: 0.2852 - val_accuracy: 0.9134\n",
      "Epoch 435/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1817 - accuracy: 0.9209 - val_loss: 0.2761 - val_accuracy: 0.9055\n",
      "Epoch 436/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.1793 - accuracy: 0.9289 - val_loss: 0.3195 - val_accuracy: 0.8976\n",
      "Epoch 437/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.1687 - accuracy: 0.9348 - val_loss: 0.3129 - val_accuracy: 0.8819\n",
      "Epoch 438/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1845 - accuracy: 0.9249 - val_loss: 0.2822 - val_accuracy: 0.8898\n",
      "Epoch 439/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1928 - accuracy: 0.9170 - val_loss: 0.2785 - val_accuracy: 0.9055\n",
      "Epoch 440/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1959 - accuracy: 0.9150 - val_loss: 0.2865 - val_accuracy: 0.8976\n",
      "Epoch 441/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2049 - accuracy: 0.9170 - val_loss: 0.2717 - val_accuracy: 0.9213\n",
      "Epoch 442/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1812 - accuracy: 0.9150 - val_loss: 0.2957 - val_accuracy: 0.9134\n",
      "Epoch 443/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1781 - accuracy: 0.9269 - val_loss: 0.2890 - val_accuracy: 0.9134\n",
      "Epoch 444/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2031 - accuracy: 0.9150 - val_loss: 0.3323 - val_accuracy: 0.8976\n",
      "Epoch 445/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1834 - accuracy: 0.9269 - val_loss: 0.2805 - val_accuracy: 0.9134\n",
      "Epoch 446/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2162 - accuracy: 0.9130 - val_loss: 0.3092 - val_accuracy: 0.9055\n",
      "Epoch 447/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1778 - accuracy: 0.9348 - val_loss: 0.2962 - val_accuracy: 0.9055\n",
      "Epoch 448/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1728 - accuracy: 0.9249 - val_loss: 0.2814 - val_accuracy: 0.8898\n",
      "Epoch 449/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1683 - accuracy: 0.9348 - val_loss: 0.3014 - val_accuracy: 0.8976\n",
      "Epoch 450/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1852 - accuracy: 0.9209 - val_loss: 0.2751 - val_accuracy: 0.9055\n",
      "Epoch 451/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2083 - accuracy: 0.9229 - val_loss: 0.2999 - val_accuracy: 0.9055\n",
      "Epoch 452/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.1762 - accuracy: 0.9249 - val_loss: 0.3163 - val_accuracy: 0.8976\n",
      "Epoch 453/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2214 - accuracy: 0.9209 - val_loss: 0.2661 - val_accuracy: 0.8976\n",
      "Epoch 454/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1798 - accuracy: 0.9249 - val_loss: 0.2942 - val_accuracy: 0.9055\n",
      "Epoch 455/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1921 - accuracy: 0.9150 - val_loss: 0.2982 - val_accuracy: 0.9055\n",
      "Epoch 456/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2078 - accuracy: 0.9130 - val_loss: 0.2979 - val_accuracy: 0.9055\n",
      "Epoch 457/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1799 - accuracy: 0.9209 - val_loss: 0.2901 - val_accuracy: 0.8976\n",
      "Epoch 458/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1889 - accuracy: 0.9051 - val_loss: 0.3264 - val_accuracy: 0.9134\n",
      "Epoch 459/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1756 - accuracy: 0.9229 - val_loss: 0.2930 - val_accuracy: 0.9213\n",
      "Epoch 460/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1746 - accuracy: 0.9348 - val_loss: 0.3016 - val_accuracy: 0.9055\n",
      "Epoch 461/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1939 - accuracy: 0.9289 - val_loss: 0.3284 - val_accuracy: 0.8819\n",
      "Epoch 462/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2095 - accuracy: 0.9012 - val_loss: 0.2900 - val_accuracy: 0.9134\n",
      "Epoch 463/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1925 - accuracy: 0.9308 - val_loss: 0.3090 - val_accuracy: 0.9134\n",
      "Epoch 464/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.1826 - accuracy: 0.9269 - val_loss: 0.2881 - val_accuracy: 0.9134\n",
      "Epoch 465/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2057 - accuracy: 0.9111 - val_loss: 0.3108 - val_accuracy: 0.8976\n",
      "Epoch 466/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.1840 - accuracy: 0.9368 - val_loss: 0.2817 - val_accuracy: 0.9134\n",
      "Epoch 467/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1750 - accuracy: 0.9289 - val_loss: 0.2797 - val_accuracy: 0.8976\n",
      "Epoch 468/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1701 - accuracy: 0.9289 - val_loss: 0.2981 - val_accuracy: 0.8976\n",
      "Epoch 469/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1782 - accuracy: 0.9289 - val_loss: 0.3303 - val_accuracy: 0.8898\n",
      "Epoch 470/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2126 - accuracy: 0.9130 - val_loss: 0.3097 - val_accuracy: 0.8898\n",
      "Epoch 471/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1893 - accuracy: 0.9130 - val_loss: 0.2726 - val_accuracy: 0.8976\n",
      "Epoch 472/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1964 - accuracy: 0.9289 - val_loss: 0.3256 - val_accuracy: 0.9055\n",
      "Epoch 473/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2237 - accuracy: 0.9150 - val_loss: 0.2883 - val_accuracy: 0.9134\n",
      "Epoch 474/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2088 - accuracy: 0.9170 - val_loss: 0.2866 - val_accuracy: 0.9134\n",
      "Epoch 475/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1839 - accuracy: 0.9289 - val_loss: 0.3177 - val_accuracy: 0.8898\n",
      "Epoch 476/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1694 - accuracy: 0.9190 - val_loss: 0.2988 - val_accuracy: 0.9134\n",
      "Epoch 477/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1761 - accuracy: 0.9328 - val_loss: 0.3090 - val_accuracy: 0.9055\n",
      "Epoch 478/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1640 - accuracy: 0.9249 - val_loss: 0.2918 - val_accuracy: 0.9213\n",
      "Epoch 479/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1775 - accuracy: 0.9229 - val_loss: 0.2810 - val_accuracy: 0.9213\n",
      "Epoch 480/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1812 - accuracy: 0.9190 - val_loss: 0.2941 - val_accuracy: 0.9134\n",
      "Epoch 481/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1864 - accuracy: 0.9269 - val_loss: 0.3028 - val_accuracy: 0.9055\n",
      "Epoch 482/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.1698 - accuracy: 0.9269 - val_loss: 0.3228 - val_accuracy: 0.8976\n",
      "Epoch 483/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1882 - accuracy: 0.9269 - val_loss: 0.3020 - val_accuracy: 0.8976\n",
      "Epoch 484/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1698 - accuracy: 0.9289 - val_loss: 0.2928 - val_accuracy: 0.9055\n",
      "Epoch 485/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1674 - accuracy: 0.9427 - val_loss: 0.3009 - val_accuracy: 0.9055\n",
      "Epoch 486/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1846 - accuracy: 0.9269 - val_loss: 0.3001 - val_accuracy: 0.9134\n",
      "Epoch 487/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1682 - accuracy: 0.9368 - val_loss: 0.3091 - val_accuracy: 0.9213\n",
      "Epoch 488/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1711 - accuracy: 0.9348 - val_loss: 0.3067 - val_accuracy: 0.8976\n",
      "Epoch 489/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1635 - accuracy: 0.9289 - val_loss: 0.3012 - val_accuracy: 0.9055\n",
      "Epoch 490/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1555 - accuracy: 0.9269 - val_loss: 0.3371 - val_accuracy: 0.9055\n",
      "Epoch 491/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1598 - accuracy: 0.9308 - val_loss: 0.2895 - val_accuracy: 0.9291\n",
      "Epoch 492/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1667 - accuracy: 0.9289 - val_loss: 0.3110 - val_accuracy: 0.9213\n",
      "Epoch 493/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1728 - accuracy: 0.9348 - val_loss: 0.3536 - val_accuracy: 0.9134\n",
      "Epoch 494/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1930 - accuracy: 0.9269 - val_loss: 0.3046 - val_accuracy: 0.9291\n",
      "Epoch 495/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1455 - accuracy: 0.9348 - val_loss: 0.3122 - val_accuracy: 0.9055\n",
      "Epoch 496/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.1498 - accuracy: 0.9348 - val_loss: 0.3232 - val_accuracy: 0.8898\n",
      "Epoch 497/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1619 - accuracy: 0.9249 - val_loss: 0.3005 - val_accuracy: 0.9134\n",
      "Epoch 498/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1844 - accuracy: 0.9229 - val_loss: 0.3428 - val_accuracy: 0.8976\n",
      "Epoch 499/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1818 - accuracy: 0.9249 - val_loss: 0.2712 - val_accuracy: 0.9134\n",
      "Epoch 500/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2006 - accuracy: 0.9190 - val_loss: 0.3123 - val_accuracy: 0.8976\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3123 - accuracy: 0.8976\n",
      "Test accuracy: 0.8976377844810486\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# x_train_tf = np.array([data.reshape(-1, 128, 87, 1) for data in train_df[\"acoustic_data\"]])\n",
    "\n",
    "# x_test_tf = np.array([data.reshape(-1, 128, 87, 1) for data in test_df[\"acoustic_data\"]])\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "\n",
    "model = Sequential()\n",
    "#\n",
    "model.add(Conv2D(filters = 8, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (128,8,1)))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "#\n",
    "model.add(Conv2D(filters = 16, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "# fully connected\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation = \"softmax\"))\n",
    "\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0005, beta_1=0.95, beta_2=0.999)\n",
    "\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# In tóm tắt mô hình\n",
    "model.summary()\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "history = model.fit(x_train_reshaped, y_train_encoded, epochs=500, validation_data=(x_test_reshaped, y_test_encoded))\n",
    "\n",
    "# Đánh giá mô hình\n",
    "test_loss, test_acc = model.evaluate(x_test_reshaped, y_test_encoded)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
